Highest Priority mistakes
1. discretising or imputing or converting to dummies on the entire data set instead of just doing it on training data. insert a if else to seperate the taining and testing
2. Train and test sets do not leave gap(as long as the prediction horizon - 60 days for example) in between those sets periods to account for outcome to happen


3. Score thresholds are used to generate precision recall numbers and compare models instead of comparing them on % of projects that are classified as 1 (above a threshold)

Medium priority:
1. throwing away columns that could be useful
2. using all models High: DT,LR,RF. Medium: Bagging, Boosting(adaboosting, gradient boosting), Extras Treess. low: svm, KNN, NB.
3. using meaningful parameters for each model and varying them

very minor mistakes:
label is 1 for the result that you are interested in.

Repo and Codeing:
1.repo readme
2.modularity - each model is a separate function. Each parameter is a separate loop within that function
3.hard coded/not reusable
4.repeated code
5.not commented

Don't use

selectbestK

gridsearchCV


About the percentage. we only care about the rankings not the scores. we need to caculate the number of items using k percentage, and sort the scores of descedingly and cutoff the rankings at the k percentage, lable the item above the cutoff, label all of them as 1. lable the items below the cutoff line as 0.Then get the precision and recall from the results from the results.

The theredshold is meaningless. it's just a number that used to get the labels.
